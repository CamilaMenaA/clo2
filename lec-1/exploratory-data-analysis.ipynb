{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "deea54e8",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"http://sct.inf.utfsm.cl/wp-content/uploads/2020/04/logo_di.png\" style=\"width:60%\">\n",
    "    <h1> INF-280 - Estadística Computacional </h1>\n",
    "    <h2> Análisis de datos exploratorio </h2>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fe8bdd",
   "metadata": {},
   "source": [
    "## Contenidos\n",
    "\n",
    "* [Introducción](#intro)\n",
    "* [Reglamento](#rules)\n",
    "* [Contexto Data Science](#ds)\n",
    "* [Toolbox](#toolbox)\n",
    "* [Experiencia](#experience)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf138bb",
   "metadata": {},
   "source": [
    "<div id='intro' />\n",
    "\n",
    "## Introducción\n",
    "\n",
    "Los laboratorios de estadística computacional (LEC) tienen como objetivo principal analizar datos utilizando técnicas de visualización y evidenciar el comportamiento estocástico de experimentos aleatorios mediante simulaciones computacionales. \n",
    "\n",
    "Las experiencias buscan medir la habilidad de programación en Python, la capacidad de análisis y la comprensión de docs/artículos/papers/.\n",
    "\n",
    "Recuerde que los laboratorios tienen una ponderación de 40% en la nota final del ramo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbdc63b",
   "metadata": {},
   "source": [
    "<div id='reglamento' />\n",
    "\n",
    "## Reglamento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a6e5d8",
   "metadata": {},
   "source": [
    "1. El desarrollo de los laboratorios debe ser en **Python**.\n",
    "2. El formato de entrega es un **archivo .ipynb**, es decir, un jupyter notebook.\n",
    "3. El nombre del archivo de entrega del laboratorio $i$ debe seguir el siguiente formato: *lec-i-nombregrupo.ipynb*.\n",
    "4. Se recomienda seguir las recomendaciones de estilo descritas en [PEP 8](https://www.python.org/dev/peps/pep-0008/) para su codigo.\n",
    "5. El tiempo para la realización de los laboratorios es extenso, por lo que solo se recibirán entregas hasta las 23:59 del día de entrega **a menos que se especifique lo contrario**. Entregas fuera del plazo serán calificadas con nota 0.\n",
    "6. Antes de entregar su laboratorio verifique su **reproducibilidad**. Laboratorios con errores a la hora de ejecutarse serán penalizados con descuentos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd852df",
   "metadata": {},
   "source": [
    "<div id='ds' />\n",
    "\n",
    "## Contexto Ciencia de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db791d7f",
   "metadata": {},
   "source": [
    "La ciencia de datos es un **campo interdisciplinario** que mediante un conjunto de principios, definiciones de problemas, algoritmos y procesos busca **extraer patrones no obvios** de grandes conjuntos de datos. Las habilidades más demandadas en el contexto de la ciencia de datos son:\n",
    "\n",
    "1. Programación.\n",
    "2. Bases de datos.\n",
    "3. Estadística.\n",
    "4. Probabilidad.\n",
    "5. Machine learning.\n",
    "\n",
    "A lo largo de los laboratorios se experimentará computacionalmente con la **estadística** y la **probablidad**, además se desarrollara un nivel más avanzado de **programacion** en Python mediante algunas poderosas librerías.\n",
    "\n",
    "El análisis de datos es una tarea crucial en todo proyecto de ciencia de datos, generalmente consiste en las siguientes tareas:\n",
    "\n",
    "1. Data collection.\n",
    "2. Data preprocessing.\n",
    "3. Exploratory data analysis.\n",
    "4. Communication.\n",
    "\n",
    "Es importante recalcar que este proceso no es lineal, sino que más bien iterativo. A lo largo de los laboratorios, y en particular en este, realizaremos múltiples análisis de datos.\n",
    "\n",
    "Los LEC conforman una importante instancia de acercamiento a la ciencia de datos. Espero la disfruten."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6eaf3f4",
   "metadata": {},
   "source": [
    "<div id='toolbox' />\n",
    "\n",
    "## Toolbox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fb20ca",
   "metadata": {},
   "source": [
    "La caja de herramientas (stack de tecnologias) está conformada por:\n",
    "1. [Python](https://www.python.org/doc/).\n",
    "2. [Numpy](https://numpy.org/doc/stable/).\n",
    "3. [Pandas](https://pandas.pydata.org/docs/).\n",
    "4. [SciPy](https://docs.scipy.org/doc/scipy/reference/stats.html).\n",
    "5. [Matplotlib](https://matplotlib.org/stable/contents.html).\n",
    "6. [Plotly](https://plotly.com/python/).\n",
    "7. [Deep note](https://deepnote.com) (*).\n",
    "\n",
    "(*) Deep note es un poderoso notebook (a pesar de estar en versión Beta) que permite el trabajo colaborativo sin los problemas de Google Colab asociados a la edición simultánea. Usted es libre de utilizar o no Deep note. Algunas interesantes alternativas son [Cocalc](https://cocalc.com/), [Colab](https://colab.research.google.com/) y por supuesto [Anaconda](https://www.anaconda.com/products/individual#Downloads). En el caso de trabajar remotamente utilizando Anaconda, se recomienda utilizar un sistema de control de versiones como Github.\n",
    "\n",
    "Las librerías se introducirán amigablemente, no se asume ningún tipo de conocimiento previo en las tecnologías presentes en nuestra toolbox salvo por Python (IWI-131), sin embargo, se busca promover la lectura de documentaciones de librerías."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273db9be",
   "metadata": {},
   "source": [
    "<div id='experience' />\n",
    "\n",
    "## Experiencia\n",
    "\n",
    "En el presente laboratorio realizaremos un análisis de datos asociado a la pandemia de COVID-19 en Chile. El objetivo es realizar un **reporte estadistico** que evidencie la **evolución de la pandemia en Chile** a través de datos y que permita **describir la situación actual del país*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81a452a",
   "metadata": {},
   "source": [
    "### 1. Importacion de librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63c3c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1049ad6c",
   "metadata": {},
   "source": [
    "### 2. Data collection\n",
    "\n",
    "Los datos se han obtenido directamente desde el [repositorio](https://github.com/MinCiencia/Datos-COVID19.git) de la **Mesa de Datos COVID-19** liderada por el Ministerio de Ciencia, Tecnología, Conocimiento e Innovación de Chile, con el objetivo realizar un reporte estadistico utilizando datos recientes. Si bien usted puede descargar los datasets directamente desde el repositorio recién mecionado, es de vital importancia que utilice las versiones de los datasets que están disponible en el repositorio del [LEC](https://github.com/diegoquezadac/lec) para realizar su laboratorio.\n",
    "\n",
    "Para ejecutar un **comando Bash** en un Jupyter Notebook es necesario comenzar la instrucción con un signo de exclamación: \"!\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db49869",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone https://github.com/diegoquezadac/lec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3f4b69",
   "metadata": {},
   "source": [
    "De los aproximadamente 100 datasets disponibles en el repositorio de la Mesa de Datos COVID 19 solo usaremos los siguientes cuatro:\n",
    "\n",
    "1. Casos totales por region.\n",
    "2. Examenes de PCR por region.\n",
    "3. Pacientes COVID-19 en UCI por region.\n",
    "4. Fallecidos con COVID-19 por region.\n",
    "\n",
    "Utilizando la función *read_csv* de Pandas cargaremos los datasets, estos se almacenarán como [dataframes](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) en nuestro codigo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a001f31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "casos_totales = pd.read_csv('datasets/CasosTotalesCumulativo_T.csv') \n",
    "examenes_pcr = pd.read_csv('datasets/PCR_T.csv') \n",
    "pacientes_uci = pd.read_csv('datasets/UCI_T.csv') \n",
    "fallecidos = pd.read_csv('datasets/FallecidosCumulativo_T.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1061bd1a",
   "metadata": {},
   "source": [
    "### 3. Explorando Pandas y los datasets (20 pts.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b53d0d0",
   "metadata": {},
   "source": [
    "Comenzaremos nuestra exploración visualizando el dataset de casos totales con el método head() de la clase DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2707af5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "casos_totales.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ee6f77",
   "metadata": {},
   "source": [
    "Notamos que las columnas son las regiones de Chile, por otro lado, cada fila es un registro de los **casos acumulados** por región. Podemos notar también que la primera columna tiene un nombre incorrecto, debemos cambiarlo para evitar futuros errores y trabajar con un DataFrame ordenado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dac1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "casos_totales = casos_totales.rename(columns = {\"Region\":'Fecha'})\n",
    "examenes_pcr = examenes_pcr.rename(columns = {\"Region\":'Fecha'})\n",
    "pacientes_uci = pacientes_uci.rename(columns = {\"Region\":'Fecha'})\n",
    "fallecidos = fallecidos.rename(columns = {\"Region\":'Fecha'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d62692e",
   "metadata": {},
   "source": [
    "Podemos obtener un importante resumen estadístico de un DataFrame utilizando el método describe() de la clase DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41632e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "casos_totales.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3502140",
   "metadata": {},
   "source": [
    "3.1) ¿Qué indica la fila **count** ?, ¿podría variar entre las columas de un mismo dataset?, si es así, ¿por qué?. **(5 pts.)** \n",
    "\n",
    "**Respuesta:**\n",
    "\n",
    "3.2) Salvo por la fila count, ¿ es util el reporte estadístico que nos ofrece el método describe() para el dataset de casos totales?, ¿por qué?. **(5 pts.)**  \n",
    "\n",
    "**Respuesta:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be002406",
   "metadata": {},
   "source": [
    "Para extraer estadisticos simples de los contagiados por día de una región en particular podríamos tomar una columna de datos y llevarla a un Numpy Array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d267469a",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = \"Valparaíso\"\n",
    "datos_region = casos_totales[region].values\n",
    "print(f\"En {region} {int(datos_region[-1])} personas han sido reportadas como portadoras de COVID-19\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73eae80b",
   "metadata": {},
   "source": [
    "Podemos visualizar el dataset a partir del día en que la región metropolitana superó los 600000 casos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff2c617",
   "metadata": {},
   "outputs": [],
   "source": [
    "casos_totales[casos_totales['Metropolitana'] > 600000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2a83db",
   "metadata": {},
   "source": [
    "3.3) Defina una función que reciba un dataframe de casos totales, un número $n$ de casos totales y una región como parámetro, la función debe retornar el día en que se alcanzaron $n$ casos totales en la región dada como párametro. Se debe manejar **adecuadamente** el caso en que aún no se hayan alcanzado $n$ casos totales para la región indicada. **(5 pts.)** \n",
    "\n",
    "**Hint 1**: Retornar un tipo de dato *datetime.datetime* es conveniente.\n",
    "\n",
    "**Hint 2**: La palabra reservada de Python *raise* permite forzar excepciones.\n",
    "\n",
    "**Respuesta:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683ef370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def dia_n_casos_totales(casos_totales, n, region):\n",
    "    \n",
    "    data = ...\n",
    "    flag =  ...\n",
    "    \n",
    "    if(flag):\n",
    "        \n",
    "        year, month, day = ...\n",
    "        \n",
    "        return datetime.datetime(year, month, day)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d74f21",
   "metadata": {},
   "source": [
    "3.4) Defina una función que reciba un dataframe de casos totales, números $n$ y $m$ de caso totales (donde $m > n$) y una región como parámetro, la función debe retornar la diferencia de días entre la fecha en la que se alcanzaron $n$ casos y la fecha en la que se alcanzaron $m$ casos en la región indicada. \n",
    "\n",
    "En Magallanes ¿cuántos días pasaron entre el día en que hubieron 10.000 y 20.000 casos totales en la región? **(5 pts.)** \n",
    "\n",
    "**Respuesta:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e62817",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dias_entre_casos(casos_totales, n, m, region):\n",
    "    \n",
    "    diferencia = ...\n",
    "    \n",
    "    return diferencia.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882a9e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "dias_entre_casos(casos_totales, 10000, 20000, \"Magallanes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dda5ba",
   "metadata": {},
   "source": [
    "### 4. Data Preproccesing (35 pts.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f451261b",
   "metadata": {},
   "source": [
    "El proceso de preprocesamiento de datos se resume en la frase \"Garbage in, Garbage out\". Antes de realizar un análisis de datos exploratorio y generar reportes estadísticos es necesario asegurarse que la data con la que se trabaja es de calidad. Debemos \"pulir\" los datos. \n",
    "\n",
    "Comenzaremos analizando el dataframe de examenes_pcr:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e5b025",
   "metadata": {},
   "outputs": [],
   "source": [
    "examenes_pcr.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cdf0ee",
   "metadata": {},
   "source": [
    "4.1) Las primeras dos filas no corresponden a datos de exámenes pcr, lo mismo ocurre en el dataframe pacientes_uci. Actualice estos dos dataframes eliminando las filas indicadas: **(2 pts.)**\n",
    "\n",
    "*Recuerde que el primer registro se realizo el dia 2020-04-09.*.\n",
    "\n",
    "**Respuesta:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e04317",
   "metadata": {},
   "outputs": [],
   "source": [
    "examenes_pcr = ...\n",
    "pacientes_uci = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16082947",
   "metadata": {},
   "source": [
    "4.2) ¿ Qué dificultades podrían haber generado las filas recién eliminadas ?. **(3 pts.)**\n",
    "\n",
    "**Respuesta:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a94bada",
   "metadata": {},
   "source": [
    "4.3) ¿ Qué dificultades genera realizar un análisis de datos en un dataset con valores NaN o Null ?.  **(3 pts)**\n",
    "\n",
    "**Respuesta:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de72b1cf",
   "metadata": {},
   "source": [
    "4.4) Hay muchas formas de evitar los valores NaN/Null. La más simple es eliminar un fila completamente si es que existe al menos un dato Nan/Null en ella, esta opción es particularmente útil cuando se cuenta con una inmensa cantidad de datos. Lamentablemente, nuestros datos son pocos y además al eliminar un registro perderíamos la  linealidad de los registros. La otra alternativa es llenar esos datos mediante algún método.\n",
    "\n",
    "Plantee 5 métodos para llenar los valores NaN del dataframe examenes_pcr, para cada método indique las posibles repercusiones estadísticas que este tendría sobre los datos. **(10 pts.)**\n",
    "\n",
    "**Respuesta:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca427a5",
   "metadata": {},
   "source": [
    "4.5) Utilizando el método que minimiza las repercusiones estadísticas actualice el dataframe examenes_pcr reemplazando los valores NaN. **(5 pts.)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2a7e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "examenes_pcr = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55fac49",
   "metadata": {},
   "source": [
    "4.6) Cerciórese que en ninguno de los dataframes existen valores NaN. Defina una función booleana que reciba como parámetro una lista de dataframes e indique si existe algún dato NaN en ellos. **(2 pts.)**\n",
    "\n",
    "**Respuesta:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d623b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hay_valores_nan(dataframes):\n",
    "    return any( ... )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d542bc",
   "metadata": {},
   "source": [
    "4.7) Antes de comenzar con el análisis de datos exploratorio es necesario crear un nuevo dataframe a partir de casos_totales para tener un registro de frecuencias absolutas y no acumuladas, es decir, para saber el número de casos por día. Defina una función que reciba el dataframe de casos totales (acumulados) como parámetro y retorne un dataframe de casos por día. **(10 pts.)**\n",
    "\n",
    "**Respuesta:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f316152",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_casos_por_dia(casos_totales):\n",
    "    rows = list()\n",
    "    for index, row in casos_totales.sort_index(ascending=False).iterrows():\n",
    "        if(index > 0):\n",
    "            i_row = ...\n",
    "        else:\n",
    "            i_row = ...\n",
    "        rows.append(i_row)\n",
    "\n",
    "    rows = rows[::-1]\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8e8bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "casos_por_dia = obtener_casos_por_dia(casos_totales)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f090a013",
   "metadata": {},
   "source": [
    "### 6. Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848533f4",
   "metadata": {},
   "source": [
    "### 7. Communication"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
